{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967e2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8d8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"quwsarohi/NanoAgent-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def inference(messages, max_new_tokens=256, temperature=0.3, min_p=0.15, **kwargs):\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        min_p=0.15,\n",
    "        temperature=temperature,\n",
    "        **kwargs\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0][inputs.shape[1] :], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3bec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but as an AI, I don't have a name. I'm designed to provide information and answer questions about the system. I'm here to help with any queries you might have related to the system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hi! Do you have a name?\"}]\n",
    "print(inference(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6278507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have the capability to book flights. My current function allows me to perform web searches for a query. If you need help with that, feel free to ask!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOOLS = [\n",
    "    {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Performs a web search for a query and returns a string of the top search results formatted as markdown with titles, links, and descriptions.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The keyword-rich detailed search query to perform.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "TOOL_TEMPLATE = \"\"\"You are a helpful AI assistant. You have a set of possible functions/tools inside <tools></tools> tags. \n",
    "Based on question, you may need to make one or more function/tool calls to answer user.\n",
    "\n",
    "You have access to the following tools/functions:\n",
    "<tools>{tools}</tools>\n",
    "\n",
    "For each function call, return a JSON list object with function name and arguments within <tool_call></tool_call> tags.\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": TOOL_TEMPLATE.format(tools=json.dumps(TOOLS))},\n",
    "    {\"role\": \"user\", \"content\": \"Can you book a flight?\"},\n",
    "]\n",
    "\n",
    "print(inference(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45dce930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the scientific name of yellow toadflax?\n",
      "LLM: <tool_call>[{\"name\": \"web_search\", \"arguments\": {\"query\": \"yellow toadflax\"}}]</tool_call>\n",
      "\n",
      "--- Web Result ---\n",
      "{\"search_result\": \"Linaria vulgaris, the common toadflax, yellow toadflax or butter-and-eggs, is a species of flowering plant in the family Plantaginaceae, native to Europe, Siberia and Central Asia. It has also been introduced and is now common in North America.\"}\n",
      "-------------------\n",
      "\n",
      "LLM: The scientific name of yellow toadflax is Linaria vulgaris. It is a species of flowering plant in the family Plantaginaceae, native to Europe, Siberia and Central Asia. It has also been introduced and is now common in North America.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def wiki_search(inp: str):\n",
    "    import wikipedia\n",
    "    result = None\n",
    "    try:\n",
    "        result = wikipedia.summary(inp)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        options = e.options\n",
    "        results = []\n",
    "        for option in options:\n",
    "            results.append(f\"# Topic: {option}\\n{wikipedia.summary(option)}\")\n",
    "        result = \"\\n\".join(results)\n",
    "    return json.dumps({\"search_result\": result.strip()})\n",
    "\n",
    "\n",
    "def extract_toolcall(input_str):\n",
    "    import re\n",
    "\n",
    "    calls = re.findall(r\"<tool_call>(.*?)</tool_call>\", input_str, re.DOTALL)\n",
    "    if calls:\n",
    "        calls = calls[-1].strip()\n",
    "        try:\n",
    "            return json.loads(calls)\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": TOOL_TEMPLATE.format(tools=json.dumps(TOOLS))},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the scientific name of yellow toadflax?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"User:\", messages[-1][\"content\"])\n",
    "llm_tool_call = inference(messages)\n",
    "print(\"LLM:\", llm_tool_call)\n",
    "\n",
    "search_query = extract_toolcall(llm_tool_call)[0][\"arguments\"][\"query\"]\n",
    "search_result = wiki_search(search_query)\n",
    "tool_reply = f\"<tool_response>{search_result}</tool_response>\"\n",
    "\n",
    "print(\"--- Web Result ---\")\n",
    "print(search_result)\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "messages += [\n",
    "    {\"role\": \"assistant\", \"content\": llm_tool_call},\n",
    "    {\"role\": \"user\", \"content\": tool_reply},\n",
    "]\n",
    "\n",
    "print(\"LLM:\", inference(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec522dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
